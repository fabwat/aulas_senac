{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Spark\n",
    "\n",
    "\n",
    "## Introdução "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como Lidar Com a Computação e Armazenamento de Big Data?\n",
    "\n",
    "\n",
    "=> **Hadoop**\n",
    "\n",
    "*  HDFS --> Armazenamento Distribuído\n",
    "\n",
    "\n",
    "* Map Reduce --> Framework computacional\n",
    "\n",
    "\n",
    "* YARN --> Gerenciador de Recursos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dificuldades do MapReduce\n",
    "\n",
    "=> É difícil decompor problemas no paradigma MapReduce\n",
    "\n",
    "\n",
    "=> Em alguns casos não é possível"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é o Apache Spark?\n",
    "\n",
    "(segundo os criadores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"350\" src=\"http://spark.apache.org/\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"350\" src=\"http://spark.apache.org/\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O que é o Apache Spark?\n",
    "\n",
    "(Em uma frase)\n",
    "\n",
    "=> O Apache Spark é um mecanismo de propósito geral para processar dados em grande escala que roda em um cluster de computadores\n",
    "\n",
    "\n",
    "=> O Apache Spark pode ser decomposto em duas partes:\n",
    "\n",
    "1. Um mecanismo para computação em clusters de computadores\n",
    "\n",
    "\n",
    "2. Um conjunto de bibliotecas, APIs e Domain Specific Languages (DSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/the-apache-spark.png\" width=\"100%\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"img/the-apache-spark.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mais sobre o Apache Spark\n",
    "\n",
    "=> Criado em 2009, como um projeto da AMPLab na universidade da California, Berkeley.\n",
    "\n",
    "\n",
    "=> Desde o início o Spark foi otimizado para rodar em memória, ajudando a processar dados mais rapidamente que abordagens alternativas como o MapReduce do Hadoop.\n",
    "\n",
    "\n",
    "=> Os criadores alegam que o Spark pode rodar até 100 vezes mais rápido que o MapReduce do Hadoop. Dez vezes mais rápido, quando o Spark trabalha em modo similar ao MapReduce do Hadoop\n",
    "\n",
    "\n",
    "=> Vale lembrar que a comparação não é exatamente justa, uma vez que a velocidade tende a ser mais importante nos casos de uso típicos do Spark do que nos processamentos batch, em que as soluções MapReduce ainda são muito boas\n",
    "\n",
    "\n",
    "=> O Spark se tornou um projeto encubado pela Apache Software Foundation em 2013. Ainda no começo de 2014, o Apache Spark foi promovido a um dos projetos mais importantes da Fundação Apache.\n",
    "\n",
    "\n",
    "=> Apesar do Spark ser um *engine* usado em uma vasta gama de aplicações (propósito geral), o Spark é normalmente associado a tarefas que demandem queries interativas em grandes datasets, processamento de dados streaming (ex: sensores ou sistemas financeiros) e tarefas de aprendizado computacional (machine learning)\n",
    "\n",
    "=> Possui muitas bibliotecas e APIs para desenvolvimento\n",
    "\n",
    "\n",
    "=> Possui amplo suporte a diversas linguagens de programação como: Java, Python, R e Scala. \n",
    "\n",
    "\n",
    "=> Além do HDFS, o Spark também pode ser integrado a sistemas de storage como HBase, Cassandra, MapR-DB, MongoDB e Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Por que o Apache Spark?\n",
    "\n",
    "=> Abstrai a programação distribuída\n",
    "\n",
    "* Em algumas situações, o desenvolvedor tem a sensação de estar trabalhando com um banco de dados\n",
    "\n",
    "\n",
    "* Em outras, o desenvolvedor trabalhará com coleções como em Python ou Scala\n",
    "\n",
    "\n",
    "=> Simplicidade: Acesso a APIs ricas e bem documentadas. Comunidade de desenvolvimento ativa\n",
    "\n",
    "\n",
    "=> Velocidade: Em 2014 foi usado para vencer o Daytona Gray Sort benchmarking challenge, processando 100 terabytes de dados armazenados em SSDs em apenas 23 minutos. O ganhador anterior utilizou o Hadoop e uma configuração diferente do cluster, mas levou 72 minutos\n",
    "\n",
    "\n",
    "=> A disputa consistia em processar conjuntos de dados estáticos. O Spark pode ser ainda mais rápido se os dados estiverem na memória  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quem usa do Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"img/who-uses-spark.jpg\" width=\"100%\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src=\"img/who-uses-spark.jpg\" width=\"100%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop vs Spark\n",
    "\n",
    "\n",
    "=> O Spark não é um substituto para o Hadoop. O MapReduce não está morto como muitos assumiram desde o aparecimento do Spark\n",
    "\n",
    "\n",
    "=> O Spark pode rodar no Hadoop, beneficiando-se do gerenciamento do cluster (YARN) e do armazenamento (HDFS, HBase etc)\n",
    "\n",
    "\n",
    "=> O Spark pode rodar fora do Hadoop, integrando-se com gerenciamento de clusters alternativos como Mesos, e plataformas de armazenamento alternativas como Cassandra e Amazon S3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"100%\" height=\"350\" src=\"https://blog.davidmantilla.com/2014/11/09/map-reduce-is-dead-long-live-spark/\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"350\" src=\"https://blog.davidmantilla.com/2014/11/09/map-reduce-is-dead-long-live-spark/\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
