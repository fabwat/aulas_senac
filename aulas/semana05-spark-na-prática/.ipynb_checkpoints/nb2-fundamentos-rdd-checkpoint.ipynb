{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Spark com Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD: `map`, `filter` e `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como obter o `SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"P2\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo o conjunto de dados de análise reduzido\n",
    "\n",
    "Usaremos um conjunto reduzido de dados (10%) da Copa KDD de 1999, que contém quase meio milhão de registros\n",
    "\n",
    "O arquivo é fornecido como um *Gzip*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "f = request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar esse arquivo para criar nosso RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo_bruto = \"./kddcup.data_10_percent.gz\"\n",
    "rdd = sc.textFile(arquivo_bruto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformação `filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa transformação pode ser aplicada aos RDDs para manter apenas os elementos que satisfazem uma determinada condição. Mais especificamente, uma função é avaliada em cada elemento no RDD original. O RDD resultante conterá apenas os elementos que fazem a função retornar `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, imagine que queremos contar quantas interações `normais` que temos em nosso conjunto de dados. Podemos filtrar nosso RDD da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_rdd = rdd.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos contar quantos elementos temos no novo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_rdd.count()\n",
    "tt = time() - t0\n",
    "print(\"Existem {} interações 'normais'\".format(normal_count))\n",
    "print(\"Contagem concluída em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que temos um total de 494.021 em nosso conjunto de dados. Podemos ver que 97.278 contém a palavra `normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que medimos o tempo decorrido para contar os elementos no RDD. Fizemos isso porque queríamos apontar que computações reais (distribuídas) no Spark acontecem quando executamos *ações* e não *transformações*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformação `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a transformação `map` no Spark, podemos aplicar uma função a todos os elementos do nosso RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, queremos ler nosso arquivo de dados como um arquivo formatado em CSV. Podemos fazer isso aplicando uma função lambda a cada elemento no RDD da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "csv_dados = rdd.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "cabecalho = csv_dados.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Ação completada em {} segundos\".format(round(tt,3)))\n",
    "pprint(cabecalho[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se pegarmos muitos elementos em vez de apenas os primeiros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "cabecalho = csv_dados.take(100000)\n",
    "tt = time() - t0\n",
    "print(\"Ação completada em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora mais tempo. Isso porque a função `map` é aplicada agora de maneira distribuída a muitos elementos no RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o `map` e funções predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que podemos usar funções pré-definidas com o `map`. Imagine que queremos ter cada elemento no RDD como um par de valores-chave em que a chave é a tag (por exemplo, *normal*) e o valor é toda a lista de elementos que representa a linha no arquivo CSV. Nós poderíamos proceder da seguinte forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_chave_valor(linha):\n",
    "    todos_itens = linha.split(\",\")\n",
    "    item_normal = todos_itens[41]\n",
    "    return (item_normal, todos_itens)\n",
    "\n",
    "chave_valor_csv = rdd.map(em_chave_valor)\n",
    "cabecalho = chave_valor_csv.take(5)\n",
    "pprint(cabecalho[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A ação `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `collect` irá colocar todos os elementos do RDD na memória. Por esse motivo, ele deve ser usado com cuidado, especialmente quando se trabalha com grandes RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados coletados em 2.769 segundos\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "todos_dados = rdd.collect()\n",
    "tt = time() - t0\n",
    "print(\"Dados coletados em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que esse processamento demorou mais que as outras ações. Cada nó de processamento do Spark que possui uma parte do RDD precisa de gerenciamento na recuperação das informações e então 'juntar' `reduce` tudo novamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último exemplo que combina todos os anteriores, queremos coletar todas as interações `normal` como pares de valor-chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém os dados de um arquivo\n",
    "arq_dados = \"./kddcup.data_10_percent.gz\"\n",
    "meu_rdd = sc.textFile(arq_dados)\n",
    "\n",
    "# transforma em chave valor\n",
    "chave_valor_csv = meu_rdd.map(em_chave_valor)\n",
    "\n",
    "# filtra por interações normais\n",
    "interacoes_normais = chave_valor_csv.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# coleta tudo e calcula o tempo\n",
    "t0 = time()\n",
    "todos_normais = interacoes_normais.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(todos_normais)\n",
    "print(\"Dados coletados em {} segundos\".format(round(tt,3)))\n",
    "print(\"Existem {} interações 'normal'\".format(normal_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta contagem corresponde à contagem anterior para interações `normal`. O novo procedimento é mais demorado. Isso ocorre porque recuperamos todos os dados com `collect` e usamos o `len` do Python na lista resultante. Antes estávamos contando apenas o número total de elementos no RDD usando `count`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
