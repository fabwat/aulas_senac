{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P02: Introdução ao Spark com Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD: `map`, `filter` e `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como obter o `SparkContext`\n",
    "\n",
    "O `SparkContext` Carregado automaticamente quando o notebook é iniciado pelo PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somente necessário quando o notebook não é carregado pelo PySpark\n",
    "#import pyspark\n",
    "#sc = pyspark.SparkContext(appName=\"P2\")\n",
    "\n",
    "# Quando o notebook é carregado pelo PySpark, a variável sc é disponibilizada automaticamente\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo o conjunto de dados de análise reduzido\n",
    "\n",
    "Usaremos um conjunto reduzido de dados (10%) da Copa KDD de 1999, que contém quase meio milhão de registros\n",
    "\n",
    "O arquivo é fornecido como um *Gzip*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "f = request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção! Lembre-se de colocar o arquivo baixado no `HDFS` (caso ainda não tenha feito). Além disso, inicie o `HDFS` e o `Yarn`.\n",
    "\n",
    "```bash\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "hdfs dfs -put kddcup.data_10_percent.gz /usr/hduser\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar esse arquivo para criar nosso RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = \"./kddcup.data_10_percent.gz\"\n",
    "rdd = sc.textFile(nome_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformação `filter`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa transformação pode ser aplicada aos RDDs para manter apenas os elementos que satisfazem uma determinada condição. Mais especificamente, uma função é avaliada em cada elemento no RDD original. O RDD resultante conterá apenas os elementos que fazem a função retornar `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, imagine que queremos contar quantas interações `normais` que temos em nosso conjunto de dados. Podemos filtrar nosso RDD da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_rdd = rdd.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora podemos contar quantos elementos temos no novo RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "contagem_normal = normal_rdd.count()\n",
    "tt = time() - t0\n",
    "print(\"Existem {} interações 'normais'\".format(contagem_normal))\n",
    "print(\"Contagem concluída em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que temos um total de 494.021 em nosso conjunto de dados. Podemos ver que 97.278 contém a palavra `normal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que medimos o tempo decorrido para contar os elementos no RDD. Fizemos isso porque queríamos apontar que computações reais (distribuídas) no Spark acontecem quando executamos *ações* e não *transformações*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A transformação `map`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando a transformação `map` no Spark, podemos aplicar uma função a todos os elementos do nosso RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso, queremos ler nosso arquivo de dados como um arquivo formatado em CSV. Podemos fazer isso aplicando uma função lambda a cada elemento no RDD da seguinte maneira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "csv_dados = rdd.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "elementos = csv_dados.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Ação completada em {} segundos\".format(round(tt,3)))\n",
    "pprint(elementos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E se pegarmos muitos elementos em vez de apenas os primeiros?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "elementos = csv_dados.take(100000)\n",
    "tt = time() - t0\n",
    "print(\"Ação completada em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demora mais tempo. Isso porque a função `map` é aplicada agora de maneira distribuída a muitos elementos no RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o `map` e funções predefinidas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claro que podemos usar funções pré-definidas com o `map`. Imagine que queremos ter cada elemento no RDD como um par de valores-chave em que a chave é a tag (por exemplo, *normal*) e o valor é toda a lista de elementos que representa a linha no arquivo CSV. Nós poderíamos proceder da seguinte forma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def em_chave_valor(linha):\n",
    "    todos_itens = linha.split(\",\")\n",
    "    chave = todos_itens[41]\n",
    "    return (chave, todos_itens)\n",
    "\n",
    "chave_valor_csv = rdd.map(em_chave_valor)\n",
    "elementos = chave_valor_csv.take(5)\n",
    "pprint(elementos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A ação `collect`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ação `collect` irá colocar todos os elementos do RDD na memória. Por esse motivo, ele deve ser usado com cuidado, especialmente quando se trabalha com grandes RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "todos_elementos = rdd.collect()\n",
    "tt = time() - t0\n",
    "print(\"Dados coletados em {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note que esse processamento demorou mais que as outras ações. Cada nó de processamento do Spark que possui uma parte do RDD precisa de gerenciamento na recuperação das informações e então 'juntar' `reduce` tudo novamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como último exemplo que combina todos os anteriores, queremos coletar todas as interações `normal` como pares de valor-chave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém os dados de um arquivo\n",
    "nome_arquivo = \"./kddcup.data_10_percent.gz\"\n",
    "meu_rdd = sc.textFile(nome_arquivo)\n",
    "\n",
    "# transforma em chave valor\n",
    "chave_valor_rdd = meu_rdd.map(em_chave_valor)\n",
    "\n",
    "# filtra por interações normais\n",
    "normais_rdd = chave_valor_rdd.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# coleta tudo e calcula o tempo\n",
    "t0 = time()\n",
    "normais = normais_rdd.collect()\n",
    "tt = time() - t0\n",
    "contagem_normais = len(normais)\n",
    "\n",
    "print(\"Dados coletados em {} segundos\".format(round(tt,3)))\n",
    "print(\"Existem {} interações 'normal'\".format(contagem_normais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta contagem corresponde à contagem anterior para interações `normal`. O novo procedimento é mais demorado. Isso ocorre porque recuperamos todos os dados com `collect` e usamos o `len` do Python na lista resultante. Antes estávamos contando apenas o número total de elementos no RDD usando `count`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
