{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Spark com Python\n",
    "\n",
    "## Trabalhando com pares RDD chave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Spark fornece funções específicas para lidar com RDDs cujos elementos são chave-valor. Normalmente são utilizaedos para fazer agregações e outros tipos de processamento por chave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Como obter o `SparkContext`\n",
    "\n",
    "O `SparkContext` Carregado automaticamente quando o notebook é iniciado pelo PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somente necessário quando o notebook não é carregado pelo PySpark\n",
    "#import pyspark\n",
    "#sc = pyspark.SparkContext(appName=\"P6\")\n",
    "\n",
    "# Quando o notebook é carregado pelo PySpark, a variável sc é disponibilizada automaticamente\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtendo o conjunto de dados de análise reduzido\n",
    "\n",
    "Usaremos um conjunto reduzido de dados (10%) da Copa KDD de 1999, que contém quase meio milhão de registros. O arquivo é fornecido como um *Gzip*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request as request\n",
    "f = request.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\", \"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção! Lembre-se de colocar o arquivo baixado no `HDFS` (caso ainda não tenha feito). Além disso, inicie o `HDFS` e o `Yarn`.\n",
    "\n",
    "```bash\n",
    "start-dfs.sh\n",
    "start-yarn.sh\n",
    "hdfs dfs -put kddcup.data_10_percent.gz /usr/hduser\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nome_arquivo = \"./kddcup.data_10_percent.gz\"\n",
    "rdd = sc.textFile(nome_arquivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um par RDD para tipos de interação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faremos alguma análise de dados exploratória em nosso conjunto de dados. Mais especificamente, queremos traçar um perfil de cada tipo de interação de rede considerando algumas variáveis como duração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rdd = rdd.map(lambda x: x.split(\",\"))\n",
    "elementos_por_chave_rdd = csv_rdd.map(lambda x: (x[41], x)) # x[41] contém o tipo de interação de rede"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now our key/value pair data ready to be used. Let's get the first element in order to see how it looks like.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementos_por_chave_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregações de dados com pares RDD chave-valor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos usar todas as transformações e ações disponívies para RDDs normais com pares de RDDs chave-valor. Só precisamos fazer com que as funções trabalhem com pares de elementos. Além disso, o Spark fornece funções específicas para trabalhar com RDDs contendo pares de elementos. São similares àqueles disponíveis para RDDs em geral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, temos uma transformação `reduceByKey` que calcula a duração total de cada tipo de interação de rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duracao_interacoes_rdd = csv_rdd.map(lambda x: (x[41], float(x[0]))) \n",
    "duracao_interacoes_por_tipo_rdd = duracao_interacoes_rdd.reduceByKey(lambda x, y: x + y)\n",
    "duracao_interacoes_por_tipo_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos especificar a ação de contagem para pares de chave-valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contagem_por_chave = elementos_por_chave_rdd.countByKey()\n",
    "contagem_por_chave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando o `combineByKey`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta é a mais geral das funções de agregação por chave. A maioria dos outros combinadores por chave utilizam essa implementação. Podemos até pensar nesta função como um equivalente ao `aggregate` já que o usuário retorna os valores que não são do mesmo tipo que a entrada de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por exemplo, podemos usar para calcular a duração média por tipo como a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interacoes_combinadas_rdd = duracao_interacoes_rdd.combineByKey(\n",
    "    (lambda x: (x, 1)), # o valor inicial com valor x e contagem 1\n",
    "    (lambda acum, valor: (acum[0]+valor, acum[1]+1)), # como combinar um valor com o acumulador: soma valor, incrementa contagem\n",
    "    (lambda acum1, acum2: (acum1[0]+acum2[0], acum1[1]+acum2[1])) # combina acumuladores\n",
    ")\n",
    "\n",
    "interacoes_combinadas_rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que os argumentos são similares ao passados ao `aggregate` apresentado anteriormente. O resultado associado a cada tipo é na forma de um par. Se quisermos realmente as médias, precisamos fazer a divisão antes de coletar os resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duracao_media_por_tipo_rdd = interacoes_combinadas_rdd.map((lambda x: (x[0], round(x[1][0]/x[1][1],3))))\n",
    "duracao_media_por_tipo_rdd.collectAsMap()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
